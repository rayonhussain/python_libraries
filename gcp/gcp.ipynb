{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Initialize a Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "# Access a specific bucket\n",
    "bucket_name = \"your-bucket-name\"\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# List all blobs (files) in the bucket\n",
    "blobs = bucket.list_blobs()\n",
    "for blob in blobs:\n",
    "    print(blob.name)\n",
    "\n",
    "# Download a blob\n",
    "blob_name = \"your-file.txt\"\n",
    "blob = bucket.blob(blob_name)\n",
    "blob.download_to_filename(\"local-file.txt\")\n",
    "\n",
    "# Upload a file\n",
    "blob.upload_from_filename(\"local-file.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BQ Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Write a query\n",
    "query = \"\"\"\n",
    "    SELECT name, age\n",
    "    FROM `your-project-id.your-dataset.your-table`\n",
    "    WHERE age > 30\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "query_job = client.query(query)\n",
    "\n",
    "# Fetch results\n",
    "results = query_job.result()\n",
    "for row in results:\n",
    "    print(f\"Name: {row['name']}, Age: {row['age']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data in BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define table details\n",
    "table_id = \"your-project-id.your-dataset.your-table\"\n",
    "\n",
    "# Load data from a local CSV file\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1, autodetect=True\n",
    ")\n",
    "with open(\"local-file.csv\", \"rb\") as source_file:\n",
    "    load_job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "\n",
    "# Wait for the load job to complete\n",
    "load_job.result()\n",
    "\n",
    "# Verify the loaded data\n",
    "table = client.get_table(table_id)\n",
    "print(f\"Loaded {table.num_rows} rows into {table_id}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd Party API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# API URL and authentication details\n",
    "api_url = \"https://api.thirdparty.com/data\"\n",
    "headers = {\"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\"}\n",
    "\n",
    "# Fetch data from the API\n",
    "response = requests.get(api_url, headers=headers)\n",
    "\n",
    "# Process the response\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access DB's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# Database connection details\n",
    "connection = pymysql.connect(\n",
    "    host=\"your-database-host\",\n",
    "    user=\"your-username\",\n",
    "    password=\"your-password\",\n",
    "    database=\"your-database-name\"\n",
    ")\n",
    "\n",
    "# Execute a query\n",
    "with connection.cursor() as cursor:\n",
    "    query = \"SELECT * FROM your_table WHERE age > %s\"\n",
    "    cursor.execute(query, (30,))  # Passing parameters securely\n",
    "    results = cursor.fetchall()\n",
    "    for row in results:\n",
    "        print(row)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize Pub/Sub subscriber\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "subscription_path = \"projects/your-project-id/subscriptions/your-subscription-name\"\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client()\n",
    "table_id = \"your-project-id.your-dataset.your-table\"\n",
    "\n",
    "# Callback function for Pub/Sub messages\n",
    "def callback(message):\n",
    "    print(f\"Received message: {message.data}\")\n",
    "\n",
    "    # Convert message to BigQuery format\n",
    "    row = {\"column1\": \"value1\", \"column2\": \"value2\"}\n",
    "    errors = bq_client.insert_rows_json(table_id, [row])\n",
    "    if not errors:\n",
    "        print(\"Data successfully written to BigQuery\")\n",
    "    else:\n",
    "        print(f\"Errors: {errors}\")\n",
    "    message.ack()\n",
    "\n",
    "# Subscribe to the topic\n",
    "future = subscriber.subscribe(subscription_path, callback=callback)\n",
    "print(f\"Listening for messages on {subscription_path}...\")\n",
    "\n",
    "# Keep the subscriber active\n",
    "try:\n",
    "    future.result()\n",
    "except KeyboardInterrupt:\n",
    "    future.cancel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataflow(ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "# Define the pipeline\n",
    "options = PipelineOptions()\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    (pipeline\n",
    "     | \"Read from GCS\" >> beam.io.ReadFromText(\"gs://your-bucket/input.csv\")\n",
    "     | \"Parse CSV\" >> beam.Map(lambda line: line.split(\",\"))\n",
    "     | \"Write to GCS\" >> beam.io.WriteToText(\"gs://your-bucket/output\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

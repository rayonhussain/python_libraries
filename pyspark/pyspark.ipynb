{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YourAppName\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# From a list of rows\n",
    "data = [Row(name=\"Alice\", age=25), Row(name=\"Bob\", age=30)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# From RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df_from_rdd = spark.createDataFrame(rdd)\n",
    "\n",
    "# From CSV/JSON/Parquet files\n",
    "csv_df = spark.read.csv(\"file_path\", header=True, inferSchema=True)\n",
    "json_df = spark.read.json(\"file_path\")\n",
    "parquet_df = spark.read.parquet(\"file_path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data\n",
    "df.show()\n",
    "\n",
    "# Schema and data types\n",
    "df.printSchema()\n",
    "df.dtypes\n",
    "\n",
    "# Select columns\n",
    "df.select(\"name\", \"age\").show()\n",
    "\n",
    "# Filter rows\n",
    "df.filter(df.age > 25).show()\n",
    "\n",
    "# Add or modify columns\n",
    "df.withColumn(\"new_col\", df.age + 5).show()\n",
    "\n",
    "# Drop a column\n",
    "df.drop(\"age\").show()\n",
    "\n",
    "# Grouping and Aggregation\n",
    "df.groupBy(\"name\").agg({\"age\": \"mean\"}).show()\n",
    "\n",
    "# Sorting\n",
    "df.orderBy(df.age.desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing DF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "df.write.csv(\"output_path\", header=True)\n",
    "\n",
    "# Save as JSON\n",
    "df.write.json(\"output_path\")\n",
    "\n",
    "# Save as Parquet\n",
    "df.write.parquet(\"output_path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Transformations\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "filtered_rdd = rdd.filter(lambda x: x > 2)\n",
    "\n",
    "# Actions\n",
    "collected = rdd.collect()\n",
    "count = rdd.count()\n",
    "first_element = rdd.first()\n",
    "\n",
    "# Aggregate\n",
    "sum_all = rdd.reduce(lambda x, y: x + y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Run SQL queries\n",
    "result_df = spark.sql(\"SELECT name, age FROM people WHERE age > 25\")\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Prepare data\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
    "data = assembler.transform(df).select(\"features\", \"label\")\n",
    "\n",
    "# Train-test split\n",
    "train, test = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(train)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.transform(test)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "schema = StructType().add(\"name\", \"string\").add(\"age\", \"integer\")\n",
    "\n",
    "# Read from a stream source\n",
    "stream_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"streaming_input_path\")\n",
    "\n",
    "# Process and write stream\n",
    "query = stream_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Spark Configurations\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF's(User Defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Define a UDF\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "square_udf = udf(square, IntegerType())\n",
    "\n",
    "# Apply UDF\n",
    "df.withColumn(\"squared\", square_udf(df.age)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when, count, sum, avg\n",
    "\n",
    "df.select(col(\"age\") + 1).show()\n",
    "df.withColumn(\"is_adult\", when(col(\"age\") >= 18, lit(\"Yes\")).otherwise(lit(\"No\"))).show()\n",
    "df.groupBy(\"name\").agg(count(\"*\").alias(\"count\"), sum(\"age\").alias(\"total_age\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcasted_var = spark.sparkContext.broadcast([1, 2, 3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulator = spark.sparkContext.accumulator(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"checkpoint_dir\")\n",
    "rdd.checkpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark complex data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, avg, count, max\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "# Create a Spark session which acts as the entry point for using PySpark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Manipulation Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load Data\n",
    "# Create a sample DataFrame for demonstration\n",
    "data = [\n",
    "    (\"Alice\", 25, \"F\", 5000),\n",
    "    (\"Bob\", 30, \"M\", 6000),\n",
    "    (\"Charlie\", 35, \"M\", 7000),\n",
    "    (\"Diana\", 40, \"F\", 8000),\n",
    "    (\"Eve\", 45, \"F\", 9000),\n",
    "]\n",
    "columns = [\"Name\", \"Age\", \"Gender\", \"Salary\"]\n",
    "\n",
    "# Convert the Python list into a PySpark DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Display the data\n",
    "print(\"Original Data:\")\n",
    "df.show()\n",
    "\n",
    "# Step 3: Transformations\n",
    "\n",
    "# 3.1: Add a new column with conditional logic\n",
    "# Add a column \"Age Group\" based on the age value\n",
    "df = df.withColumn(\n",
    "    \"Age Group\", \n",
    "    when(col(\"Age\") < 30, \"Young\")\n",
    "    .when(col(\"Age\").between(30, 40), \"Middle-aged\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "# 3.2: Filter rows\n",
    "# Keep only rows where Salary > 6000\n",
    "filtered_df = df.filter(col(\"Salary\") > 6000)\n",
    "\n",
    "# 3.3: Aggregate data\n",
    "# Group by Gender and calculate average Salary and maximum Age\n",
    "aggregated_df = df.groupBy(\"Gender\").agg(\n",
    "    avg(\"Salary\").alias(\"Avg Salary\"),\n",
    "    max(\"Age\").alias(\"Max Age\")\n",
    ")\n",
    "\n",
    "# 3.4: Rename a column\n",
    "# Rename the column \"Salary\" to \"Income\"\n",
    "renamed_df = df.withColumnRenamed(\"Salary\", \"Income\")\n",
    "\n",
    "# 3.5: Drop a column\n",
    "# Drop the \"Age Group\" column\n",
    "dropped_df = df.drop(\"Age Group\")\n",
    "\n",
    "# Step 4: Save the manipulated data\n",
    "# Write the filtered DataFrame to a CSV file (example path)\n",
    "# Make sure the output directory is writable\n",
    "filtered_df.write.csv(\"output/filtered_data\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Step 5: Display Results\n",
    "\n",
    "print(\"Data with Age Group column added:\")\n",
    "df.show()\n",
    "\n",
    "print(\"Filtered Data (Salary > 6000):\")\n",
    "filtered_df.show()\n",
    "\n",
    "print(\"Aggregated Data (Average Salary and Max Age by Gender):\")\n",
    "aggregated_df.show()\n",
    "\n",
    "print(\"Data with Renamed Column (Salary -> Income):\")\n",
    "renamed_df.show()\n",
    "\n",
    "print(\"Data with Age Group Column Dropped:\")\n",
    "dropped_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Join\n",
    "inner_join_df = df1.join(df2, on=\"Name\", how=\"inner\")\n",
    "\n",
    "# Left Outer Join\n",
    "left_join_df = df1.join(df2, on=\"Name\", how=\"left\")\n",
    "\n",
    "# Full Outer Join\n",
    "full_join_df = df1.join(df2, on=\"Name\", how=\"outer\")\n",
    "\n",
    "# Display results\n",
    "print(\"Inner Join Result:\")\n",
    "inner_join_df.show()\n",
    "\n",
    "print(\"Left Outer Join Result:\")\n",
    "left_join_df.show()\n",
    "\n",
    "print(\"Full Outer Join Result:\")\n",
    "full_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Alice\", \"HR\", 5000), (\"Bob\", \"IT\", 6000), \n",
    "        (\"Charlie\", \"HR\", 4500), (\"Diana\", \"IT\", 7000),\n",
    "        (\"Eve\", \"HR\", 5500)]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a Window Specification\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(\"Salary\")\n",
    "\n",
    "# Apply Window Functions\n",
    "df = df.withColumn(\"Rank\", rank().over(window_spec))\n",
    "df = df.withColumn(\"Dense Rank\", dense_rank().over(window_spec))\n",
    "df = df.withColumn(\"Row Number\", row_number().over(window_spec))\n",
    "\n",
    "# Display results\n",
    "print(\"Data with Window Functions:\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Alice\", \"HR\", 5000), (\"Bob\", \"IT\", 6000), \n",
    "        (\"Charlie\", \"HR\", 4500), (\"Diana\", \"IT\", 7000),\n",
    "        (\"Eve\", \"HR\", 5500)]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "pivot_df = df.groupBy(\"Department\").pivot(\"Name\").sum(\"Salary\")\n",
    "\n",
    "# Display results\n",
    "print(\"Pivoted DataFrame:\")\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Alice\", None, 5000), (\"Bob\", \"IT\", None), \n",
    "        (\"Charlie\", \"HR\", 4500), (None, \"IT\", 7000)]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Fill null values\n",
    "df_filled = df.fillna({\"Name\": \"Unknown\", \"Salary\": 0})\n",
    "\n",
    "# Drop rows with null values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "# Replace specific null values\n",
    "df_replaced = df.na.replace({\"IT\": \"Information Technology\"})\n",
    "\n",
    "# Display results\n",
    "print(\"DataFrame with Nulls Filled:\")\n",
    "df_filled.show()\n",
    "\n",
    "print(\"DataFrame with Nulls Dropped:\")\n",
    "df_dropped.show()\n",
    "\n",
    "print(\"DataFrame with Replaced Values:\")\n",
    "df_replaced.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Sample DataFrame with arrays\n",
    "data = [(\"Alice\", [5000, 6000]), (\"Bob\", [7000, 8000]), (\"Charlie\", [])]\n",
    "columns = [\"Name\", \"Salaries\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Explode array column into multiple rows\n",
    "exploded_df = df.withColumn(\"Salary\", explode(col(\"Salaries\")))\n",
    "\n",
    "# Display results\n",
    "print(\"Exploded DataFrame:\")\n",
    "exploded_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrames\n",
    "data1 = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "data2 = [(\"Charlie\", 35), (\"Diana\", 40)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"Name\", \"Age\"])\n",
    "df2 = spark.createDataFrame(data2, [\"Name\", \"Age\"])\n",
    "\n",
    "# Union of DataFrames\n",
    "union_df = df1.union(df2)\n",
    "\n",
    "# Display results\n",
    "print(\"Union of DataFrames:\")\n",
    "union_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Sort by a column\n",
    "sorted_df = df.orderBy(\"Age\", ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Sorted DataFrame:\")\n",
    "sorted_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Alice\", 25), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "\n",
    "# Drop duplicates based on specific columns\n",
    "df_no_duplicates_cols = df.dropDuplicates([\"Name\"])\n",
    "\n",
    "# Display results\n",
    "print(\"DataFrame without Duplicates:\")\n",
    "df_no_duplicates.show()\n",
    "\n",
    "print(\"DataFrame without Duplicates (Based on Name):\")\n",
    "df_no_duplicates_cols.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrames\n",
    "data1 = [(\"Alice\",), (\"Bob\",)]\n",
    "data2 = [(\"HR\",), (\"IT\",)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"Name\"])\n",
    "df2 = spark.createDataFrame(data2, [\"Department\"])\n",
    "\n",
    "# Cross Join\n",
    "cross_join_df = df1.crossJoin(df2)\n",
    "\n",
    "# Display results\n",
    "print(\"Cross Join DataFrame:\")\n",
    "cross_join_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35), (\"Diana\", 40)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Sample without replacement\n",
    "sampled_df = df.sample(withReplacement=False, fraction=0.5, seed=42)\n",
    "\n",
    "# Display results\n",
    "print(\"Sampled DataFrame:\")\n",
    "sampled_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
